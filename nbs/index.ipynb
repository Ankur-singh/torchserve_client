{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchServe Python Client\n",
    "\n",
    "> Python Client for TorchServe APIs\n",
    "\n",
    "Introducing \"TorchServe Client\": Your Python buddy for seamless interaction with TorchServe APIs! If you've used PyTorch to train models, TorchServe is your go-to for deploying them in production. This handy SDK simplifies the process, letting you effortlessly engage with Inference, and Management APIs. Let's tackle TorchServe together with ease! ðŸš€ðŸ”¥\n",
    "\n",
    "I've designed this package to have a seamless one-to-one mapping with the TorchServe APIs, ensuring a user-friendly experience. This project is just a lightweight wrapper, handling HTTP requests to TorchServe. By making HTTP requests to TorchServe under the hood, and abstracting away the complexity, it allows users to focus on their models and applications without getting bogged down in implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "\n",
    "Docker Command to start torchserve docker\n",
    "\n",
    "```bash\n",
    "docker run --rm -it -p 8080:8080 -p 8081:8081 -p 8082:8082 -p 7070:7070 -p 7071:7071 pytorch/torchserve:latest\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install torchserve_client\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torchserve_client` is a breeze! It has support for both REST APIs and gRPC APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REST Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make calls to REST endpoint, simply initialize a `TorchServeClientREST` object as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClient(base_url=http://localhost, management_port=8081, inference_port=8080)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "from torchserve_client import TorchServeClientREST\n",
    "\n",
    "# Initialize the REST TorchServeClient object\n",
    "ts_client = TorchServeClientREST()\n",
    "ts_client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to customize the *base URL*, *management port*, or *inference port* of your TorchServe server, you can pass them as arguments during initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClient(base_url=http://your-torchserve-server.com, management_port=8081, inference_port=8080)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "from torchserve_client import TorchServeClientREST\n",
    "\n",
    "# Customize the base URL, management port, and inference port\n",
    "ts_client = TorchServeClientREST(base_url='http://your-torchserve-server.com', \n",
    "                             management_port=8081, inference_port=8080)\n",
    "ts_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gRPC Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a gRPC client, simply create a `TorchServeClientGRPC` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClientGRPC(base_url=localhost, management_port=7071, inference_port=7070)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "from torchserve_client import TorchServeClientGRPC\n",
    "\n",
    "# Initialize the gRPC TorchServeClient object\n",
    "ts_client = TorchServeClientGRPC()\n",
    "ts_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To customize base URL and default ports, pass them as arguments during initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClientGRPC(base_url=your-torchserve-server.com, management_port=7071, inference_port=7070)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "from torchserve_client import TorchServeClientGRPC\n",
    "\n",
    "# Initialize the gRPC TorchServeClient object\n",
    "ts_client = TorchServeClientGRPC(base_url='http://your-torchserve-server.com', \n",
    "                             management_port=7071, inference_port=7070)\n",
    "ts_client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With these intuitive APIs at your disposal, you can harness the full power of the Management and Inference API and take your application to next level. Happy inferencing! ðŸš€ðŸ”¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
