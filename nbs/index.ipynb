{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchServe Python Client\n",
    "\n",
    "> Python Client for TorchServe APIs\n",
    "\n",
    "Introducing \"TorchServe Client\": Your Python buddy for seamless interaction with TorchServe APIs! If you've used PyTorch to train models, TorchServe is your go-to for deploying them in production. This handy SDK simplifies the process, letting you effortlessly engage with Inference, and Management APIs. Let's tackle TorchServe together with ease! üöÄüî•\n",
    "\n",
    "I've designed this package to have a seamless one-to-one mapping with the TorchServe APIs, ensuring a user-friendly experience. This project is just a lightweight wrapper, handling HTTP requests to TorchServe. By making HTTP requests to TorchServe under the hood, and abstracting away the complexity, it allows users to focus on their models and applications without getting bogged down in implementation details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install torchserve_client\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `torchserve_client` is a breeze! To get started, simply initialize a `TorchServeClient` object as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClient(base_url=http://localhost, management_port=8081, inference_port=8080)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchserve_client import TorchServeClient\n",
    "\n",
    "# Initialize the TorchServeClient object\n",
    "ts_client = TorchServeClient()\n",
    "ts_client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to customize the *base URL*, *management port*, or *inference port* of your TorchServe server, you can pass them as arguments during initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClient(base_url=http://your-torchserve-server.com, management_port=8081, inference_port=8080)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchserve_client import TorchServeClient\n",
    "\n",
    "# Customize the base URL, management port, and inference port\n",
    "ts_client = TorchServeClient(base_url='http://your-torchserve-server.com', \n",
    "                             management_port=8081, inference_port=8080)\n",
    "ts_client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you don't provide a base URL during initialization, the client will check for the presence of `TORCHSERVE_URL` in the environment variables. If the variable is not found, it will gracefully fall back to using *localhost* as the default. This way, you have the flexibility to tailor your TorchServeClient to your needs effortlessly! Happy serving! üçøüî•"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Management APIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TorchServe Management APIs, you can effortlessly manage your models at runtime. Here's a quick rundown of the actions you can perform using our `TorchServeClient` SDK:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Register a Model**: Easily register a model with TorchServe using the `ts_client.management.register_model()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Model \"squeezenet1_1\" Version: 1.0 registered with 0 initial workers. Use scale workers API to add workers for the model.'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.management.register_model('https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Increase/Decrease Workers**: Scale the number of workers for a specific model with simplicity using `ts_client.management.scale_workers()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Processing worker updates...'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.management.scale_workers('squeezenet1_1', min_worker=1, max_worker=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Model Status**: Curious about a model's status? Fetch all the details you need using `ts_client.management.describe_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'modelName': 'squeezenet1_1',\n",
       "  'modelVersion': '1.0',\n",
       "  'modelUrl': 'https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar',\n",
       "  'runtime': 'python',\n",
       "  'minWorkers': 1,\n",
       "  'maxWorkers': 1,\n",
       "  'batchSize': 1,\n",
       "  'maxBatchDelay': 100,\n",
       "  'loadedAtStartup': False,\n",
       "  'workers': [{'id': '9001',\n",
       "    'startTime': '2023-07-17T22:55:40.155Z',\n",
       "    'status': 'UNLOADING',\n",
       "    'memoryUsage': 0,\n",
       "    'pid': -1,\n",
       "    'gpu': False,\n",
       "    'gpuUsage': 'N/A'}]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.management.describe_model('squeezenet1_1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **List Registered Models**: Quickly fetch a list of all registered models using `ts_client.management.list_models()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'modelName': 'squeezenet1_1',\n",
       "   'modelUrl': 'https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar'}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "# List all models\n",
    "ts_client.management.list_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Set Default Model Version**: Ensure the desired version of a model is the default choice with the `ts_client.management.set_model_version()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Default vesion succsesfully updated for model \"squeezenet1_1\" to \"1.0\"'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.management.set_default_version('squeezenet1_1', '1.0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Unregister a Model**: If you need to bid farewell to a model, use the `ts_client.management.unregister_model()` function to gracefully remove it from TorchServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Model \"squeezenet1_1\" unregistered'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.management.unregister_model('squeezenet1_1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **API Description**: view a full list of Managment APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "ts_client.management.api_description()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember, all these management APIs can be accessed conveniently under the namespace `ts_client.management`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference APIs\n",
    "\n",
    "TorchServeClient allows you to interact with the Inference API, which listens on port 8080, enabling you to run inference on your samples effortlessly. Here are the available APIs under the `ts_client.inference` namespace:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **API Description**: Want to explore what APIs and options are available? Use `ts_client.inference.api_description()` to get a comprehensive list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "ts_client.inference.api_description()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Health Check API**: Ensure the health of the running server with the `ts_client.inference.health_check()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Healthy'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.inference.health_check()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Predictions API**: Get predictions from the served model using `ts_client.inference.predictions()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lynx': 0.5455798506736755,\n",
       " 'tabby': 0.2794159948825836,\n",
       " 'Egyptian_cat': 0.10391879826784134,\n",
       " 'tiger_cat': 0.06263326108455658,\n",
       " 'leopard': 0.0050191376358270645}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.inference.prediction('squeezenet1_1', data={'data': open('/Users/ankursingh/Downloads/kitten_small.jpg', 'rb')})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. **Explanations API**: Dive into the served model's explanations with ease using `ts_client.inference.explanations()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "ts_client.inference.explaination('squeezenet1_1', data={'data': open('/Users/ankursingh/Downloads/kitten_small.jpg', 'rb')})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With these intuitive APIs at your disposal, you can harness the full power of the Management and Inference API and take your application to next level. Happy inferencing! üöÄüî•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
