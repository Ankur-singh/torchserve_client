{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class BaseClient:\n",
    "    def __init__(self, base_url=None):\n",
    "        base_url = base_url if base_url else os.environ.get('TORCHSERVE_URL', 'http://localhost')\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "\n",
    "    def _make_request(self, method, endpoint, json=None, params=None, files=None):\n",
    "        \"\"\"\n",
    "        json: dict. The JSON body of the request.\n",
    "        params: dict. The URL parameters of the request.\n",
    "        files: [dict]. The files to upload.\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        response = requests.request(method, url, json=json, params=params, files=files)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(base_url={self.base_url})\"\n",
    "    \n",
    "    def _filter_none_values(self, data):\n",
    "        return {key: value for key, value in data.items() if value is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ManagementClient(BaseClient):\n",
    "    def __init__(self, base_url=None, port=8081):\n",
    "        super().__init__(base_url)\n",
    "        self.port = port\n",
    "        self.base_url = f\"{self.base_url}:{self.port}\"\n",
    "\n",
    "    def register_model(self, url, model_name=None, handler=None, runtime=None,\n",
    "                       batch_size=1, max_batch_delay=100, initial_workers=0,\n",
    "                       synchronous=False, response_timeout=120):\n",
    "        data = {\n",
    "            'url': url,\n",
    "            'model_name': model_name,\n",
    "            'handler': handler,\n",
    "            'runtime': runtime,\n",
    "            'batch_size': batch_size,\n",
    "            'max_batch_delay': max_batch_delay,\n",
    "            'initial_workers': initial_workers,\n",
    "            'synchronous': synchronous,\n",
    "            'response_timeout': response_timeout\n",
    "        }\n",
    "        data = self._filter_none_values(data)\n",
    "        return self._make_request('POST', '/models', json=data)\n",
    "\n",
    "    def scale_workers(self, model_name, version=None, min_worker=1, max_worker=None,\n",
    "                      synchronous=False, timeout=-1):\n",
    "        if version:\n",
    "            endpoint = f\"/models/{model_name}/{version}\"\n",
    "        else:\n",
    "            endpoint = f\"/models/{model_name}\"\n",
    "\n",
    "        params = {\n",
    "            'min_worker': min_worker,\n",
    "            'max_worker': max_worker if max_worker is not None else min_worker,\n",
    "            'synchronous': synchronous,\n",
    "            'timeout': timeout\n",
    "        }\n",
    "\n",
    "        return self._make_request('PUT', endpoint, json=params)\n",
    "    \n",
    "    def describe_model(self, model_name, version=None, customized=False):\n",
    "        \"\"\"\n",
    "        Returns the model description.\n",
    "        version :  is optional. if `all` return status of all version of a model. If not provided, the latest version will be returned.\n",
    "        allowed\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        if customized:\n",
    "            params['customized'] = customized\n",
    "            \n",
    "        if version:\n",
    "            endpoint = f\"/models/{model_name}/{version}\"\n",
    "        else:\n",
    "            endpoint = f\"/models/{model_name}\"\n",
    "        return self._make_request('GET', endpoint, params=params)\n",
    "    \n",
    "    def unregister_model(self, model_name, version=None):\n",
    "        if version:\n",
    "            endpoint = f\"/models/{model_name}/{version}\"\n",
    "        else:\n",
    "            endpoint = f\"/models/{model_name}\"\n",
    "\n",
    "        return self._make_request('DELETE', endpoint)\n",
    "    \n",
    "    def list_models(self, limit=100, next_page_token=None):\n",
    "        params = {\n",
    "            'limit': limit,\n",
    "            'next_page_token': next_page_token\n",
    "        }\n",
    "        params = self._filter_none_values(params)\n",
    "        return self._make_request('GET', '/models', params=params)\n",
    "    \n",
    "    def api_description(self):\n",
    "        return self._make_request('OPTIONS', '/')\n",
    "    \n",
    "    def set_default_version(self, model_name, version):\n",
    "        endpoint = f\"/models/{model_name}/{version}/set-default\"\n",
    "        return self._make_request('PUT', endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class InferenceClient(BaseClient):\n",
    "    def __init__(self, base_url=None, port=8080):\n",
    "        super().__init__(base_url)\n",
    "        self.port = port\n",
    "        self.base_url = f\"{self.base_url}:{self.port}\"\n",
    "\n",
    "    def api_description(self):\n",
    "        return self._make_request('OPTIONS', '/')\n",
    "    \n",
    "    def health_check(self):\n",
    "        return self._make_request('GET', '/ping')\n",
    "    \n",
    "    def prediction(self, model_name, data, version=None):\n",
    "        \"\"\"\n",
    "        data = [\n",
    "            ('data', open('docs/images/dogs-before.jpg', 'rb')),\n",
    "            ('data', open('docs/images/kitten_small.jpg', 'rb')),\n",
    "        ]\n",
    "        \"\"\"\n",
    "        if version:\n",
    "            endpoint = f\"/predictions/{model_name}/{version}\"\n",
    "        else:\n",
    "            endpoint = f\"/predictions/{model_name}\"\n",
    "\n",
    "        return self._make_request('POST', endpoint, files=data)\n",
    "    \n",
    "    def explaination(self, model_name, data):\n",
    "        \"\"\"\n",
    "        data <string : bytes>\n",
    "        \"\"\"\n",
    "        endpoint = f\"/explanations/{model_name}\"\n",
    "        return self._make_request('POST', endpoint, files=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TorchServeClientREST:\n",
    "    def __init__(self, base_url=None, management_port=8081, inference_port=8080):\n",
    "        self.management = ManagementClient(base_url, management_port)\n",
    "        self.inference = InferenceClient(base_url, inference_port)\n",
    "\n",
    "    def __repr__(self):\n",
    "        url = self.management.base_url.rsplit(':', 1)[0]\n",
    "        return f\"TorchServeClientREST(base_url={url}, management_port={self.management.port}, inference_port={self.inference.port})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make calls to REST endpoint, simply initialize a `TorchServeClientREST` object as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClientREST(base_url=http://localhost, management_port=8081, inference_port=8080)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "# Initialize the REST TorchServeClient object\n",
    "ts_client = TorchServeClientREST()\n",
    "ts_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to customize the *base URL*, *management port*, or *inference port* of your TorchServe server, you can pass them as arguments during initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClientREST(base_url=http://your-torchserve-server.com, management_port=8081, inference_port=8080)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "# Customize the base URL, management port, and inference port\n",
    "ts_client = TorchServeClientREST(base_url='http://your-torchserve-server.com', \n",
    "                             management_port=8081, inference_port=8080)\n",
    "ts_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you don't provide a base URL during initialization, the client will check for the presence of `TORCHSERVE_URL` in the environment variables. If the variable is not found, it will gracefully fall back to using *localhost* as the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Management APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TorchServe Management APIs, you can effortlessly manage your models at runtime. Here's a quick rundown of the actions you can perform using our `TorchServeClient` SDK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Register a Model**: Easily register a model with TorchServe using the `ts_client.management.register_model()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "ts_client.management.register_model('https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Increase/Decrease Workers**: Scale the number of workers for a specific model with simplicity using `ts_client.management.scale_workers()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Processing worker updates...'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.management.scale_workers('squeezenet1_1', min_worker=1, max_worker=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Model Status**: Curious about a model's status? Fetch all the details you need using `ts_client.management.describe_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'modelName': 'squeezenet1_1',\n",
       "  'modelVersion': '1.0',\n",
       "  'modelUrl': 'https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar',\n",
       "  'runtime': 'python',\n",
       "  'minWorkers': 1,\n",
       "  'maxWorkers': 1,\n",
       "  'batchSize': 1,\n",
       "  'maxBatchDelay': 100,\n",
       "  'loadedAtStartup': False,\n",
       "  'workers': [{'id': '9001',\n",
       "    'startTime': '2023-07-17T22:55:40.155Z',\n",
       "    'status': 'UNLOADING',\n",
       "    'memoryUsage': 0,\n",
       "    'pid': -1,\n",
       "    'gpu': False,\n",
       "    'gpuUsage': 'N/A'}]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.management.describe_model('squeezenet1_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **List Registered Models**: Quickly fetch a list of all registered models using `ts_client.management.list_models()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'modelName': 'squeezenet1_1',\n",
       "   'modelUrl': 'https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "# List all models\n",
    "ts_client.management.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Set Default Model Version**: Ensure the desired version of a model is the default choice with the `ts_client.management.set_model_version()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Default vesion succsesfully updated for model \"squeezenet1_1\" to \"1.0\"'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.management.set_default_version('squeezenet1_1', '1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Unregister a Model**: If you need to bid farewell to a model, use the `ts_client.management.unregister_model()` function to gracefully remove it from TorchServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Model \"squeezenet1_1\" unregistered'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.management.unregister_model('squeezenet1_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **API Description**: view a full list of Managment APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "ts_client.management.api_description()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember, all these management APIs can be accessed conveniently under the namespace `ts_client.management`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchServeClient allows you to interact with the Inference API, which listens on port 8080, enabling you to run inference on your samples effortlessly. Here are the available APIs under the `ts_client.inference` namespace:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **API Description**: Want to explore what APIs and options are available? Use `ts_client.inference.api_description()` to get a comprehensive list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "ts_client.inference.api_description()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Health Check API**: Ensure the health of the running server with the `ts_client.inference.health_check()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Healthy'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.inference.health_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Predictions API**: Get predictions from the served model using `ts_client.inference.predictions()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lynx': 0.5455798506736755,\n",
       " 'tabby': 0.2794159948825836,\n",
       " 'Egyptian_cat': 0.10391879826784134,\n",
       " 'tiger_cat': 0.06263326108455658,\n",
       " 'leopard': 0.0050191376358270645}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "ts_client.inference.prediction('squeezenet1_1', data={'data': open('/Users/ankursingh/Downloads/kitten_small.jpg', 'rb')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. **Explanations API**: Dive into the served model's explanations with ease using `ts_client.inference.explanations()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "ts_client.inference.explaination('squeezenet1_1', data={'data': open('/Users/ankursingh/Downloads/kitten_small.jpg', 'rb')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
