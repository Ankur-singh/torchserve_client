{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gRPC Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import grpc\n",
    "import torchserve_client.proto.inference_pb2 as inference_pb2\n",
    "import torchserve_client.proto.inference_pb2_grpc as inference_pb2_grpc\n",
    "import torchserve_client.proto.management_pb2 as management_pb2\n",
    "import torchserve_client.proto.management_pb2_grpc as management_pb2_grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class BaseClient:\n",
    "    def __init__(self, base_url=None):\n",
    "        base_url = base_url if base_url else os.environ.get('TORCHSERVE_URL', 'http://localhost')\n",
    "        self.base_url = base_url.split('//')[1]\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(base_url={self.base_url})\"\n",
    "    \n",
    "    def _filter_none_values(self, data):\n",
    "        return {key: value for key, value in data.items() if value is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ManagementClient(BaseClient):\n",
    "    def __init__(self, base_url=None, port=7071):\n",
    "        super().__init__(base_url)\n",
    "        self.port = port\n",
    "        self.base_url = f\"{self.base_url}:{self.port}\"\n",
    "        self.channel = grpc.insecure_channel(self.base_url)\n",
    "        self.stub = management_pb2_grpc.ManagementAPIsServiceStub(self.channel)\n",
    "\n",
    "    def describe_model(self, model_name, model_version=None, customized=None):\n",
    "        method_args = {key: value for key, value in locals().items() if key != \"self\"}\n",
    "        method_args = self._filter_none_values(method_args)\n",
    "        request = management_pb2.DescribeModelRequest(model_name=model_name)\n",
    "        return self.stub.DescribeModel(request)\n",
    "\n",
    "    def list_models(self, limit=None, next_page_token=None):\n",
    "        method_args = {key: value for key, value in locals().items() if key != \"self\"}\n",
    "        method_args = self._filter_none_values(method_args)\n",
    "        request = management_pb2.ListModelsRequest(**method_args)\n",
    "        return self.stub.ListModels(request)\n",
    "\n",
    "    def register_model(\n",
    "        self,\n",
    "        batch_size=None,\n",
    "        handler=None,\n",
    "        initial_workers=None,\n",
    "        max_batch_delay=None,\n",
    "        model_name=None,\n",
    "        response_timeout=None,\n",
    "        runtime=None,\n",
    "        synchronous=None,\n",
    "        url=None,\n",
    "        s3_sse_kms=None,\n",
    "    ):\n",
    "        method_args = {key: value for key, value in locals().items() if key != \"self\"}\n",
    "        method_args = self._filter_none_values(method_args)\n",
    "        request = management_pb2.RegisterModelRequest(**method_args)\n",
    "        return self.stub.RegisterModel(request)\n",
    "    \n",
    "    def scale_worker(\n",
    "        self,\n",
    "        model_name,\n",
    "        min_worker=None,\n",
    "        max_worker=None,\n",
    "        model_version=None,\n",
    "        number_gpu=None,\n",
    "        synchronous=None,\n",
    "        timeout=None,\n",
    "    ):\n",
    "        method_args = {key: value for key, value in locals().items() if key != \"self\"}\n",
    "        method_args = self._filter_none_values(method_args)\n",
    "        request = management_pb2.ScaleWorkerRequest(**method_args)\n",
    "        return self.stub.ScaleWorker(request)\n",
    "\n",
    "    def set_default(self, model_name, model_version):\n",
    "        method_args = {key: value for key, value in locals().items() if key != \"self\"}\n",
    "        method_args = self._filter_none_values(method_args)\n",
    "        request = management_pb2.SetDefaultRequest(**method_args)\n",
    "        return self.stub.SetDefault(request)\n",
    "\n",
    "    def unregister_model(self, model_name, model_version=None):\n",
    "        method_args = {key: value for key, value in locals().items() if key != \"self\"}\n",
    "        method_args = self._filter_none_values(method_args)\n",
    "        request = management_pb2.UnregisterModelRequest(**method_args)\n",
    "        return self.stub.UnregisterModel(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class InferenceClient(BaseClient):\n",
    "    def __init__(self, base_url=None, port=7070):\n",
    "        super().__init__(base_url)\n",
    "        self.port = port\n",
    "        self.base_url = f\"{self.base_url}:{self.port}\"\n",
    "        self.channel = grpc.insecure_channel(self.base_url)\n",
    "        self.stub = inference_pb2_grpc.InferenceAPIsServiceStub(self.channel)\n",
    "\n",
    "    def ping(self):\n",
    "        return self.stub.Ping()\n",
    "\n",
    "    def predictions(self, model_name, input_data, model_version=None):\n",
    "        \"\"\"\n",
    "        input_data = {\"data\": data}\n",
    "        \"\"\"\n",
    "        method_args = {key: value for key, value in locals().items() if key != \"self\"}\n",
    "        method_args = self._filter_none_values(method_args)\n",
    "        request = inference_pb2.PredictionsRequest(**method_args)     \n",
    "        return self.stub.Predictions(request)\n",
    "\n",
    "    def stream_predictions(self, model_name, input_data, model_version=None):\n",
    "        \"\"\"\n",
    "        input_data = {\"data\": data}\n",
    "        \"\"\"\n",
    "        method_args = {key: value for key, value in locals().items() if key != \"self\"}\n",
    "        method_args = self._filter_none_values(method_args)\n",
    "        request = inference_pb2.PredictionsRequest(**method_args)\n",
    "        return self.stub.StreamPredictions(request)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TorchServeClientGRPC:\n",
    "    def __init__(self, base_url=None, management_port=7071, inference_port=7070):\n",
    "        self.management = ManagementClient(base_url, management_port)\n",
    "        self.inference = InferenceClient(base_url, inference_port)\n",
    "\n",
    "    def __repr__(self):\n",
    "        url = self.management.base_url.rsplit(':', 1)[0]\n",
    "        return f\"TorchServeClientGRPC(base_url={url}, management_port={self.management.port}, inference_port={self.inference.port})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a gRPC client, simply create a `TorchServeClientGRPC` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClientGRPC(base_url=localhost, management_port=7071, inference_port=7070)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "# Initialize the gRPC TorchServeClient object\n",
    "ts_client = TorchServeClientGRPC()\n",
    "ts_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To customize base URL and default ports, pass them as arguments during initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchServeClientGRPC(base_url=your-torchserve-server.com, management_port=7071, inference_port=7070)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "# Initialize the gRPC TorchServeClient object\n",
    "ts_client = TorchServeClientGRPC(base_url='http://your-torchserve-server.com', \n",
    "                             management_port=7071, inference_port=7070)\n",
    "ts_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Management APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of all the supported gRPC management endpoints:\n",
    "\n",
    "- `describe_model`: Provide detailed information about the default version of a model\n",
    "\n",
    "    **Arguments**:\n",
    "\n",
    "    - `model_name` (str, required): Name of the model to describe\n",
    "\n",
    "    - `model_version` (str, optional): Version of the model to describe\n",
    "\n",
    "    - `customized` (bool, optional): Customized metadata\n",
    "\n",
    "    **Usage**:\n",
    "\n",
    "    ```python\n",
    "    response = ts_client.management.describe_model(model_name=\"mnist\")\n",
    "    response.msg\n",
    "    ```\n",
    "\n",
    "\n",
    "- `list_models`: List all registered models in TorchServe\n",
    "\n",
    "    **Arguments**:\n",
    "\n",
    "    - `limit` (int, optional): Maximum number of items to return (default: 100).\n",
    "\n",
    "    - `next_page_token` (int, optional): Token to retrieve the next set of results\n",
    "\n",
    "    **Usage**:\n",
    "    \n",
    "    ```python\n",
    "    response = ts_client.management.list_models()\n",
    "    response.msg\n",
    "    ```\n",
    "\n",
    "- `register_model` : Register a new model to TorchServe\n",
    "\n",
    "    **Arguments**:\n",
    "    \n",
    "    - `batch_size` (int, optional): Inference batch size (default: 1).\n",
    "    \n",
    "    - `handler` (str, optional): Inference handler entry-point.\n",
    "    \n",
    "    - `initial_workers` (int, optional): Number of initial workers (default: 0).\n",
    "    \n",
    "    - `max_batch_delay` (int, optional): Maximum delay for batch aggregation (default: 100).\n",
    "    \n",
    "    - `model_name` (str, optional): Name of the model.\n",
    "    \n",
    "    - `response_timeout` (int, optional): Maximum time for model response (default: 120 seconds).\n",
    "    \n",
    "    - `runtime` (str, optional): Runtime for model custom service code.\n",
    "    \n",
    "    - `synchronous` (bool, optional): Synchronous worker creation (default: False).\n",
    "    \n",
    "    - `url` (str, required): Model archive download URL.\n",
    "    \n",
    "    - `s3_sse_kms` (bool, optional): S3 SSE KMS enabled (default: False).\n",
    "\n",
    "    **Usage**:\n",
    "\n",
    "    ```python\n",
    "    response = ts_client.management.register_model()\n",
    "    response.msg\n",
    "    ```    \n",
    "\n",
    "- `scale_worker`: Configure the number of workers for a model. This is an asynchronous call by default\n",
    "\n",
    "    **Arguments**:\n",
    "\n",
    "    - `model_name` (str, required): Name of the model to scale workers.\n",
    "    \n",
    "    - `model_version` (str, optional): Model version.\n",
    "    \n",
    "    - `max_worker` (int, optional): Maximum number of worker processes.\n",
    "    \n",
    "    - `min_worker` (int, optional): Minimum number of worker processes.\n",
    "    \n",
    "    - `number_gpu` (int, optional): Number of GPU worker processes to create.\n",
    "    \n",
    "    - `synchronous` (bool, optional): Synchronous call (default: False).\n",
    "    \n",
    "    - `timeout` (int, optional): Wait time for worker completion (0: terminate immediately, -1: wait infinitely).\n",
    "\n",
    "    **Usage**:\n",
    "\n",
    "    ```python\n",
    "    response = ts_client.management.scale_worker()\n",
    "    response.msg\n",
    "    ```    \n",
    "\n",
    "\n",
    "- `set_default`: Set default version of a model\n",
    "\n",
    "    **Arguments**:\n",
    "\n",
    "    - `model_name` (str, required): Name of the model for which the default version should be updated\n",
    "    \n",
    "    - `model_version` (str, required): Version of the model to set as the default version\n",
    "\n",
    "    **Usage**:\n",
    "\n",
    "    ```python\n",
    "    response = ts_client.management.set_default()\n",
    "    response.msg\n",
    "    ```\n",
    "\n",
    "\n",
    "- `unregister_model`: Unregister a particular version of a model from TorchServe. This call is asynchronous by default.\n",
    "    \n",
    "    **Arguments**:\n",
    "    \n",
    "    - `model_name` (str, required): Name of the model to unregister.\n",
    "    \n",
    "    - `model_version` (str, optional): Version of the model to unregister. If none, then default version of the model will be unregistered.\n",
    "\n",
    "    **Usage**:\n",
    "\n",
    "    ```python\n",
    "    response = ts_client.management.unregister_model()\n",
    "    response.msg\n",
    "    ```    \n",
    "\n",
    "Check [`management.proto`](https://github.com/pytorch/serve/blob/master/frontend/server/src/main/resources/proto/management.proto) file to better understand the arguments of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list gRPC inference endpoints:\n",
    "\n",
    "- `ping`: Check Health Status\n",
    "\n",
    "    **Usage**:\n",
    "\n",
    "    ```python\n",
    "    response = ts_client.inference.ping()\n",
    "    response.health\n",
    "    ```\n",
    "\n",
    "- `predictions`: Get predictions\n",
    "    \n",
    "    **Arguments**:\n",
    "\n",
    "    - `model_name` (str, required): Name of the model.\n",
    "\n",
    "    - `model_version` (str, optional): Version of the model. If not provided, default version will be used.\n",
    "\n",
    "    - `input` (Dict[str, bytes], required): Input data for model prediction\n",
    "\n",
    "    **Usage**:\n",
    "    \n",
    "    ```python\n",
    "    response = ts_client.inference.predictions(model_name=\"mnist\", input={\"data\": data})\n",
    "    response.prediction.decode(\"utf-8\")\n",
    "    ```\n",
    "\n",
    "- `steam_predictions`: Get steaming predictions\n",
    "\n",
    "    **Arguments**:\n",
    "    \n",
    "    - `model_name` (str, required): Name of the model.\n",
    "    \n",
    "    - `model_version` (str, optional): Version of the model. If not provided, default version will be used.\n",
    "    \n",
    "    - `input` (Dict[str, bytes], required): Input data for model prediction\n",
    "\n",
    "    **Usage**:\n",
    "\n",
    "    ```python\n",
    "    response = ts_client.inference.stream_predictions(model_name=\"mnist\", input={\"data\": data})\n",
    "    response.prediction.decode(\"utf-8\")\n",
    "    ```\n",
    "\n",
    "Again, for more detail about gRPC request and response objects, refer [`inference.proto`](https://github.com/pytorch/serve/blob/master/frontend/server/src/main/resources/proto/inference.proto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
